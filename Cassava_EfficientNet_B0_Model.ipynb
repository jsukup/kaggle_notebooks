{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Cassava - EfficientNet B0 Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsukup/kaggle_notebooks/blob/main/Cassava_EfficientNet_B0_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t1_Bwh4jCJj",
        "outputId": "e8ff8e0c-fefe-4978-a476-1f0945cf6161",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\r\n",
        "!mkdir ./data/\r\n",
        "os.environ['KAGGLE_USERNAME'] = 'jsukup' # username from the json file\r\n",
        "os.environ['KAGGLE_KEY'] = 'ebdb5d1a622c34ff3e66a0d2e55e146c' # key from the json file\r\n",
        "!kaggle competitions download -c cassava-leaf-disease-classification -p ./data/ # api copied from kaggle"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./data/’: File exists\n",
            "usage: kaggle [-h] [-v] {competitions,c,datasets,d,kernels,k,config} ...\n",
            "kaggle: error: unrecognized arguments: --unzip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO4V57g532PP",
        "outputId": "cb0da219-68f6-4b29-c839-be945c036c06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!kaggle competitions files -c cassava-leaf-disease-classification"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "name                                    size  creationDate         \n",
            "-------------------------------------  -----  -------------------  \n",
            "train_tfrecords/ld_train06-1338.tfrec  216MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train15-1327.tfrec  214MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train03-1338.tfrec  217MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train09-1338.tfrec  217MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train08-1338.tfrec  217MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train13-1338.tfrec  217MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train11-1338.tfrec  218MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train05-1338.tfrec  214MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train07-1338.tfrec  216MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train04-1338.tfrec  216MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train12-1338.tfrec  216MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train14-1338.tfrec  215MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train01-1338.tfrec  215MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train00-1338.tfrec  217MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train02-1338.tfrec  216MB  2020-11-25 16:32:50  \n",
            "train_tfrecords/ld_train10-1338.tfrec  217MB  2020-11-25 16:32:50  \n",
            "train_images/1003888281.jpg            110KB  2020-11-25 16:32:50  \n",
            "train_images/1000201771.jpg            158KB  2020-11-25 16:32:50  \n",
            "train_images/1000812911.jpg             79KB  2020-11-25 16:32:50  \n",
            "train_images/1000837476.jpg            104KB  2020-11-25 16:32:50  \n",
            "train_images/1003987001.jpg            101KB  2020-11-25 16:32:50  \n",
            "train_images/1002088496.jpg            131KB  2020-11-25 16:32:50  \n",
            "train_images/100204014.jpg             170KB  2020-11-25 16:32:50  \n",
            "train_images/1003218714.jpg            127KB  2020-11-25 16:32:50  \n",
            "train_images/1000723321.jpg            119KB  2020-11-25 16:32:50  \n",
            "train_images/1002394761.jpg            101KB  2020-11-25 16:32:50  \n",
            "train_images/1003298598.jpg            142KB  2020-11-25 16:32:50  \n",
            "train_images/1003442061.jpg            101KB  2020-11-25 16:32:50  \n",
            "train_images/1001742395.jpg            100KB  2020-11-25 16:32:50  \n",
            "train_images/1002255315.jpg            150KB  2020-11-25 16:32:50  \n",
            "train_images/1000910826.jpg            100KB  2020-11-25 16:32:50  \n",
            "train_images/1001749118.jpg            135KB  2020-11-25 16:32:50  \n",
            "train_images/1000015157.jpg            136KB  2020-11-25 16:32:50  \n",
            "train_images/100042118.jpg              73KB  2020-11-25 16:32:50  \n",
            "train_images/1001320321.jpg             82KB  2020-11-25 16:32:50  \n",
            "train_images/1001723730.jpg             84KB  2020-11-25 16:32:50  \n",
            "test_images/2216849948.jpg             141KB  2020-11-25 16:32:50  \n",
            "test_tfrecords/ld_test00-1.tfrec       191KB  2020-11-25 16:32:50  \n",
            "label_num_to_disease_map.json           172B  2020-11-25 16:32:50  \n",
            "train.csv                              350KB  2020-11-25 16:32:50  \n",
            "sample_submission.csv                    32B  2020-11-25 16:32:50  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMu7fCLdjB5T"
      },
      "source": [
        "zips = tf.io.gfile.glob('./data/ld_*.tfrec.zip')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaNfhZ5x2GlY"
      },
      "source": [
        "import zipfile"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t_rFbkAyRhJ"
      },
      "source": [
        "for tfrecs in zips:\r\n",
        "    with zipfile.ZipFile(tfrecs, 'r') as zip_ref:\r\n",
        "        zip_ref.extractall('./data/')\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pjbahxGdm_QC"
      },
      "source": [
        "#Imports\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from functools import partial\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5FevVYaum_QK",
        "outputId": "64777651-74ca-4817-8d77-9c3face832af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Set TPU Strategy\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print(\"Device:\", tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    \n",
        "print(\"Number of replicas:\", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: grpc://10.103.162.146:8470\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.103.162.146:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.103.162.146:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of replicas: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zlFGK-V3m_QL"
      },
      "source": [
        "# Load data from Kaggle Notebook storage\n",
        "# raw_storage = '../input/cassava-leaf-disease-classification'\n",
        "\n",
        "# with open(os.path.join(raw_storage, 'label_num_to_disease_map.json')) as f:\n",
        "#     datastore = json.load(f)\n",
        "    \n",
        "# datastore = {int(k): str(v) for k, v in datastore.items()}\n",
        "        \n",
        "# raw_labels = pd.read_csv(os.path.join(raw_storage, 'train.csv'), ) \n",
        "# raw_labels['label'].replace(datastore, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8e4FH81fm_QL"
      },
      "source": [
        "# Calculate class weights\n",
        "# calc_weights = class_weight.compute_class_weight('balanced',\n",
        "#                                                   np.unique(raw_labels['label']),\n",
        "#                                                   raw_labels['label'])\n",
        "\n",
        "# class_weights = {i : calc_weights[i] for i in range(len(calc_weights))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6NT-BN_Km_QM"
      },
      "source": [
        "#HYPERPARAMETERS\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "IMG_SIZE = [224, 224]\n",
        "EPOCHS = 300\n",
        "LEARNING_RATE = 1e-2\n",
        "#GCP_DATASET = 'gs://kds-0ec1dec9f628bce2bc810157442805632bd221934a9e1921da420af8'\n",
        "RANDOM_SEED = 734\n",
        "MODEL_HUB_URL = 'https://tfhub.dev/tensorflow/efficientnet/b0/classification/1'\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "CLASS_WEIGHTS = {0: 3.9368905243790246,\n",
        "                1: 1.954956601187757,\n",
        "                2: 1.7935456831517183,\n",
        "                3: 0.3252317981456148,\n",
        "                4: 1.6606131160263873}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1Zmq3g6im_QM"
      },
      "source": [
        "# Read/load tfrecords\n",
        "def decode_image(image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    image = tf.image.resize(image, IMG_SIZE, preserve_aspect_ratio=True)\n",
        "    image = tf.reshape(image, [*IMG_SIZE, 3])\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    return image\n",
        "\n",
        "def read_tfrecord(example, labeled):\n",
        "    tfrecord_format = (\n",
        "        {\n",
        "            \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "            \"target\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        }\n",
        "        if labeled else \n",
        "        {\n",
        "            \"image\": tf.io.FixedLenFeature([], tf.string)\n",
        "        }\n",
        "    )\n",
        "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
        "    image = decode_image(example[\"image\"])\n",
        "    if labeled:\n",
        "        label = tf.cast(example[\"target\"], tf.int32)\n",
        "        return image, label\n",
        "    return image\n",
        "\n",
        "def load_dataset(filenames, labeled=True):\n",
        "    ignore_order = tf.data.Options()\n",
        "    ignore_order.experimental_deterministic = False  # disable order, increase speed\n",
        "    dataset = tf.data.TFRecordDataset(\n",
        "        filenames\n",
        "    )  # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(\n",
        "        ignore_order\n",
        "    )  # uses data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.map(\n",
        "        partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "    # returns a dataset of (image, label) pairs if labeled=True or just images if labeled=False\n",
        "    return dataset\n",
        "\n",
        "def get_dataset(filenames, labeled=True):\n",
        "    dataset = load_dataset(filenames, labeled=labeled)\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7FTdjEKHm_QN",
        "outputId": "8466118d-6b59-41ac-f73a-07678406ab77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        }
      },
      "source": [
        "# List files\n",
        "train_files, val_files = train_test_split(tf.io.gfile.glob(GCP_DATASET + '/train_tfrecords/ld_train*.tfrec'),\n",
        "                                                           test_size=0.2,\n",
        "                                                           random_state=RANDOM_SEED)\n",
        "\n",
        "test_files = tf.io.gfile.glob(GCP_DATASET + '/test_tfrecords/ld_test*.tfrec')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ada2d281295c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# List files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_files, val_files = train_test_split(tf.io.gfile.glob(GCP_DATASET + '/train_tfrecords/ld_train*.tfrec'),\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                            \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                            random_state=RANDOM_SEED)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mget_matching_files_v2\u001b[0;34m(pattern)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         for matching_filename in _pywrap_file_io.GetMatchingFiles(\n\u001b[0;32m--> 412\u001b[0;31m             compat.as_bytes(pattern))\n\u001b[0m\u001b[1;32m    413\u001b[0m     ]\n\u001b[1;32m    414\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Error executing an HTTP request: HTTP response code 404 with body '{\n  \"error\": {\n    \"code\": 404,\n    \"message\": \"Not Found\",\n    \"errors\": [\n      {\n        \"message\": \"Not Found\",\n        \"domain\": \"global\",\n        \"reason\": \"notFound\"\n      }\n    ]\n  }\n}\n'\n\t when reading gs://kds-0ec1dec9f628bce2bc810157442805632bd221934a9e1921da420af8/train_tfrecords"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Bw7ZezTUm_QN"
      },
      "source": [
        "# Get datasets\n",
        "train_dataset = get_dataset(train_files)\n",
        "val_dataset = get_dataset(val_files)\n",
        "test_dataset = get_dataset(test_files, labeled=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgYCrqmJTvuO"
      },
      "source": [
        "# Count images per dataset for additional HYPERPARAMETERS\r\n",
        "def count_data_items(filenames):\r\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\r\n",
        "    return np.sum(n)\r\n",
        "\r\n",
        "NUM_TRAINING_IMAGES = count_data_items(train_files)\r\n",
        "NUM_VALIDATION_IMAGES = count_data_items(val_files)\r\n",
        "NUM_TEST_IMAGES = count_data_items(test_files)\r\n",
        "\r\n",
        "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\r\n",
        "VALID_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Qdli-1rwm_QN"
      },
      "source": [
        "# Examine batch of images\n",
        "# image_batch, label_batch = next(iter(train_dataset))\n",
        "\n",
        "# def show_batch(image_batch, label_batch):\n",
        "#     plt.figure(figsize=(20, 20))\n",
        "#     for n in range(25):\n",
        "#         ax = plt.subplot(5, 5, n + 1)\n",
        "#         plt.imshow(image_batch[n])\n",
        "#         if label_batch[n] in datastore:\n",
        "#             plt.title(datastore[label_batch[n]], color='w')\n",
        "#         plt.axis(\"off\")\n",
        "\n",
        "\n",
        "# show_batch(image_batch.numpy(), label_batch.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kWjJI7Y2m_QO"
      },
      "source": [
        "# Callback instantiation\n",
        "lr_scheduler = ExponentialDecay(initial_learning_rate=LEARNING_RATE,\n",
        "                                decay_steps=10000,\n",
        "                                decay_rate=0.98)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                               min_delta=0.001,\n",
        "                               patience=20,\n",
        "                               verbose=1,\n",
        "                               restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zi-QWUkT2pO"
      },
      "source": [
        "# Fixes TPU error when using TF Hub models\r\n",
        "os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\"\r\n",
        "load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\r\n",
        "reloaded_model = hub.load(MODEL_HUB_URL, options=load_options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Czcf1SF9m_QO"
      },
      "source": [
        "# Create model\n",
        "def make_model():\n",
        "    print('Building model using TF Hub model:', MODEL_HUB_URL)\n",
        "    with strategy.scope():\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=[*IMG_SIZE, 3]),\n",
        "            hub.KerasLayer(MODEL_HUB_URL, trainable=False),\n",
        "            tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(5, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['sparse_categorical_accuracy'])\n",
        "        \n",
        "    return model\n",
        "\n",
        "model = make_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5nPo59Gpm_QP"
      },
      "source": [
        "# Train model\n",
        "history = model.fit(train_dataset.repeat(),\n",
        "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks=[early_stopping],\n",
        "                    validation_data=val_dataset,\n",
        "                    validation_steps=VALID_STEPS,\n",
        "                    class_weight=CLASS_WEIGHTS,\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lne7yfYKnMGf"
      },
      "source": [
        "# Visualize results\r\n",
        "plt.figure()\r\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\r\n",
        "plt.xlabel(\"Training Steps\")\r\n",
        "plt.ylim([0,1])\r\n",
        "plt.plot(history[\"accuracy\"])\r\n",
        "plt.plot(history[\"val_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}